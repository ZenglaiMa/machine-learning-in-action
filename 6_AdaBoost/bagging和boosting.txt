
Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，
形成一个性能更加强大的分类器。更准确的说这是一种分类算法的组装方法，即将弱分类器组装成强分类器的方法。       

Bagging (bootstrap aggregating)
	- 算法思想：从原始样本集中抽取新训练集，新数据集大小和原始数据集一致。每轮从原始样
		本集中有放回地抽取n个训练样本，有放回表示训练集中的有些样本可能被多次抽取到，
		而有些样本可能一次都没有被抽中。换句话说，新数据集中可能有重复的样本，而原始
		数据集中的某些样本在新数据集中则不会再出现。共进行k轮抽取，得到k个训练集，
		k个训练集之间是相互独立的。每次使用一个训练集得到一个模型，k个训练集共得到k个模型，
		并且认为得到的这k个模型的重要性相同。对分类问题，将得到的k个模型采用投票多数表决
		的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。
		
Boosting
	- 其主要思想是将弱分类器组装成一个强分类器，在PAC（概率近似正确）学习框架下，一定可以将弱分类器组装成一个强分类器。
	- 两个核心问题
		1. 在每一轮如何改变训练数据的权值或概率分布？
			提高那些在前一轮被弱分类器错分的样例的权值，减小前一轮分对的样例的权值，
			来使得分类器对误分的数据有较好的效果。
		2. 通过什么方式来组合弱分类器？
			通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，
			即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
			而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。
			
Bagging和Boosting的区别
	1. 样本选择上
		Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各个训练集之间是独立的。
		Boosting：每一轮的训练集不变，变的是训练集中每个样例在分类器中的权值，权值根据上一轮的分类结果进行调整。
	2. 训练时样例地权值
		Bagging：使用均匀取样，每个样例的权重相等。
		Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
	3. 得到的分类器的权重
		Bagging：所有分类器的权重相等。
		Boosting：每个弱分类器都有相应的权重，分类误差越小的分类器会有越大的权重。
	4. 并行计算
		Bagging：各个分类器可以并行生成。
		Boosting：各个分类器只能串行生成，因为后一个模型参数需要前一轮模型的结果。

总结       
	Bagging和Boosting都是把若干个弱分类器整合为一个强分类器的方法，
	只是整合的方式不一样，最终得到不一样的效果。将不同的分类算法套
	入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，
	但是也增大了计算量。下面是将决策树与这些算法框架进行结合所得到
	的新的算法：
		Bagging + 决策树 = 随机森林
		AdaBoost + 决策树 = 提升树
		Gradient Boosting + 决策树 = GBDT
		
		
		
		
		
		
		
		
		
		
